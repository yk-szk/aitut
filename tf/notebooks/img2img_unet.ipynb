{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bone suppression(U-Net)\n",
    "参考：[Pix2Pix  |  TensorFlow Core](https://www.tensorflow.org/tutorials/generative/pix2pix#training)\n",
    "\n",
    "## 前準備\n",
    "### 主要パッケージのインポート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "from logging import basicConfig, getLogger, INFO\n",
    "basicConfig(level=INFO, format='%(asctime)s %(levelname)s :%(message)s')\n",
    "logger = getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データディレクトリの指定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT = pathlib.Path('Data/Images/chest_xray')\n",
    "INPUT_IMAGE_DIR = 'bone_enhancement'\n",
    "TRUTH_IMAGE_DIR = 'bone_suppression'\n",
    "CLASS_LABELS = ('lung')\n",
    "IMAGE_EXT = '.png'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 画像ファイルを基にpd.DataFrameを作成する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset_df(data_root, input_dir, truth_dir, image_ext):\n",
    "    dfs = []\n",
    "    root = pathlib.Path(data_root)\n",
    "    image_filenames = (root / pathlib.Path(input_dir)).glob('*' + image_ext)\n",
    "    df = pd.DataFrame(image_filenames, columns=['input_path'])\n",
    "    df['truth_path'] = df['input_path'].map(\n",
    "        lambda p: root / pathlib.Path(truth_dir) / p.name)\n",
    "    return df\n",
    "\n",
    "\n",
    "df_dataset = create_dataset_df(DATA_ROOT, INPUT_IMAGE_DIR, TRUTH_IMAGE_DIR,\n",
    "                               IMAGE_EXT)\n",
    "display(df_dataset.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 画像を表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_SAMPLES = 3\n",
    "plt.figure(figsize=(10, 5))\n",
    "for i, sample in enumerate(df_dataset.sample(n=N_SAMPLES).itertuples()):\n",
    "    image = Image.open(sample.input_path)\n",
    "    truth = Image.open(sample.truth_path)\n",
    "    plt.figure(figsize=(4, 2))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(image, cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(truth, cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデル作成\n",
    "構造はhttps://www.tensorflow.org/tutorials/generative/pix2pix のgeneratorを使う。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def downsample(filters, size, apply_batchnorm=True):\n",
    "    initializer = tf.random_normal_initializer(0., 0.02)\n",
    "\n",
    "    result = tf.keras.Sequential()\n",
    "    result.add(\n",
    "        tf.keras.layers.Conv2D(filters,\n",
    "                               size,\n",
    "                               strides=2,\n",
    "                               padding='same',\n",
    "                               kernel_initializer=initializer,\n",
    "                               use_bias=False))\n",
    "\n",
    "    if apply_batchnorm:\n",
    "        result.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "    result.add(tf.keras.layers.LeakyReLU())\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def upsample(filters, size, apply_dropout=False):\n",
    "    initializer = tf.random_normal_initializer(0., 0.02)\n",
    "\n",
    "    result = tf.keras.Sequential()\n",
    "    result.add(\n",
    "        tf.keras.layers.Conv2DTranspose(filters,\n",
    "                                        size,\n",
    "                                        strides=2,\n",
    "                                        padding='same',\n",
    "                                        kernel_initializer=initializer,\n",
    "                                        use_bias=False))\n",
    "\n",
    "    result.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "    if apply_dropout:\n",
    "        result.add(tf.keras.layers.Dropout(0.5))\n",
    "\n",
    "    result.add(tf.keras.layers.ReLU())\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SHAPE = [256, 256, 1]\n",
    "OUTPUT_CHANNELS = 1\n",
    "\n",
    "\n",
    "def Generator():\n",
    "    inputs = tf.keras.layers.Input(shape=INPUT_SHAPE)\n",
    "\n",
    "    down_stack = [\n",
    "        downsample(64, 4, apply_batchnorm=False),  # (bs, 128, 128, 64)\n",
    "        downsample(128, 4),  # (bs, 64, 64, 128)\n",
    "        downsample(256, 4),  # (bs, 32, 32, 256)\n",
    "        downsample(512, 4),  # (bs, 16, 16, 512)\n",
    "        downsample(512, 4),  # (bs, 8, 8, 512)\n",
    "        downsample(512, 4),  # (bs, 4, 4, 512)\n",
    "        downsample(512, 4),  # (bs, 2, 2, 512)\n",
    "        downsample(512, 4),  # (bs, 1, 1, 512)\n",
    "    ]\n",
    "\n",
    "    up_stack = [\n",
    "        upsample(512, 4, apply_dropout=True),  # (bs, 2, 2, 1024)\n",
    "        upsample(512, 4, apply_dropout=True),  # (bs, 4, 4, 1024)\n",
    "        upsample(512, 4, apply_dropout=True),  # (bs, 8, 8, 1024)\n",
    "        upsample(512, 4),  # (bs, 16, 16, 1024)\n",
    "        upsample(256, 4),  # (bs, 32, 32, 512)\n",
    "        upsample(128, 4),  # (bs, 64, 64, 256)\n",
    "        upsample(64, 4),  # (bs, 128, 128, 128)\n",
    "    ]\n",
    "\n",
    "    initializer = tf.random_normal_initializer(0., 0.02)\n",
    "    last = tf.keras.layers.Conv2DTranspose(\n",
    "        OUTPUT_CHANNELS,\n",
    "        4,\n",
    "        strides=2,\n",
    "        padding='same',\n",
    "        kernel_initializer=initializer,\n",
    "        activation='linear')  # (bs, 256, 256, 3)\n",
    "\n",
    "    x = inputs\n",
    "\n",
    "    # Downsampling through the model\n",
    "    skips = []\n",
    "    for down in down_stack:\n",
    "        x = down(x)\n",
    "        skips.append(x)\n",
    "\n",
    "    skips = reversed(skips[:-1])\n",
    "\n",
    "    # Upsampling and establishing the skip connections\n",
    "    for up, skip in zip(up_stack, skips):\n",
    "        x = up(x)\n",
    "        x = tf.keras.layers.Concatenate()([x, skip])\n",
    "\n",
    "    x = last(x)\n",
    "\n",
    "    return tf.keras.Model(inputs=inputs, outputs=x)\n",
    "\n",
    "\n",
    "generator = Generator()\n",
    "tf.keras.utils.plot_model(generator, show_shapes=True, dpi=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile\n",
    "lossはMeanSquaredErrorをつかう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(1e-3),\n",
    "    loss=tf.keras.losses.MeanSquaredError(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習\n",
    "### ホールドアウト"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "N_SPLITS = 5\n",
    "SEED = 0\n",
    "kfold = KFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
    "train_index, test_index = next(kfold.split(df_dataset['input_path']))\n",
    "\n",
    "df_train = df_dataset.iloc[train_index]\n",
    "df_test = df_dataset.iloc[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "IMG_SHAPE = INPUT_SHAPE\n",
    "\n",
    "\n",
    "def load_img(filepath):\n",
    "    return np.atleast_3d(\n",
    "        tf.keras.preprocessing.image.load_img(filepath,\n",
    "                                              color_mode='grayscale',\n",
    "                                              target_size=IMG_SHAPE))\n",
    "\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    np.stack([\n",
    "        load_img(filepath) for filepath in tqdm.tqdm(df_train['input_path'])\n",
    "    ]),\n",
    "    np.stack([\n",
    "        load_img(filepath) for filepath in tqdm.tqdm(df_train['truth_path'])\n",
    "    ]),\n",
    "))\n",
    "\n",
    "print(dataset.element_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_addons as tfa\n",
    "\n",
    "\n",
    "def convert(image, truth):\n",
    "    image = tf.image.convert_image_dtype(image, tf.float32)\n",
    "    truth = tf.image.convert_image_dtype(truth, tf.float32)\n",
    "    return image, truth\n",
    "\n",
    "\n",
    "MAX_ANGLE_DEG = 10\n",
    "MAX_ANGLE_RAD = np.deg2rad(MAX_ANGLE_DEG)\n",
    "FLIP_RATE = .5\n",
    "BRIGHTNESS_RANGE = .2\n",
    "\n",
    "\n",
    "def augment(image, truth):\n",
    "    image, truth = convert(image, truth)\n",
    "    # rotate\n",
    "    angle = tf.random.uniform((), minval=-MAX_ANGLE_RAD, maxval=MAX_ANGLE_RAD)\n",
    "    image = tfa.image.rotate(image, angle, interpolation='BILINEAR')\n",
    "    truth = tfa.image.rotate(truth, angle, interpolation='BILINEAR')\n",
    "\n",
    "    # flip\n",
    "    if tf.random.uniform(()) < FLIP_RATE:\n",
    "        image = tf.image.flip_left_right(image)\n",
    "        truth = tf.image.flip_left_right(truth)\n",
    "\n",
    "    # brightness\n",
    "    brightness_offset = tf.random.uniform((),\n",
    "                                          minval=-BRIGHTNESS_RANGE,\n",
    "                                          maxval=BRIGHTNESS_RANGE)\n",
    "    image = image + brightness_offset\n",
    "    truth = truth + brightness_offset\n",
    "\n",
    "    return image, truth\n",
    "\n",
    "\n",
    "for d in dataset.map(augment):\n",
    "    plt.figure(figsize=(3, 2))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(d[0][..., 0], cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(d[1][..., 0], cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tut_utils\n",
    "\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "EPOCHS = 256\n",
    "PATIENCE = 16\n",
    "LR_PATIENCE = PATIENCE // 2\n",
    "BATCH_SIZE = 4\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='loss',\n",
    "                                                  restore_best_weights=True,\n",
    "                                                  patience=PATIENCE,\n",
    "                                                  min_delta=5e-6)\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='loss',\n",
    "                                                 factor=0.5,\n",
    "                                                 patience=LR_PATIENCE,\n",
    "                                                 min_lr=1e-5)\n",
    "\n",
    "logger.info('start of training')\n",
    "with tut_utils.ProgressBarCallback(EPOCHS,\n",
    "                                   len(train_index) // BATCH_SIZE,\n",
    "                                   BATCH_SIZE) as pbar:\n",
    "    result = generator.fit(dataset.shuffle(BATCH_SIZE * 16).map(\n",
    "        augment,\n",
    "        num_parallel_calls=AUTOTUNE).batch(BATCH_SIZE).prefetch(AUTOTUNE),\n",
    "                           epochs=EPOCHS,\n",
    "                           verbose=0,\n",
    "                           shuffle=False,\n",
    "                           callbacks=[pbar, early_stopping, reduce_lr])\n",
    "logger.info('end of training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "history = copy.copy(result.history)\n",
    "lr_history = history.pop('lr', None)\n",
    "plt.figure(figsize=(3, 4))\n",
    "pd.DataFrame(history).plot(title='Training history', figsize=(5, 3))\n",
    "plt.ylabel('loss')\n",
    "plt.ylim(0, history['loss'][1])\n",
    "if lr_history is not None:\n",
    "    plt.gca().twinx().plot(lr_history, color='pink', linestyle='--', label='lr')\n",
    "    plt.gca().set_ylabel('lr')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 評価\n",
    "準備中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "def show_images_in_a_row(images, titles, figsize=(15, 5)):\n",
    "    assert len(images) == len(titles), 'Invalid size of arguments'\n",
    "    plt.figure(figsize=figsize)\n",
    "    for i, (image, title) in enumerate(zip(images, titles), 1):\n",
    "        plt.subplot(1, len(images), i)\n",
    "        plt.imshow(image, cmap='gray')\n",
    "        plt.title(title)\n",
    "        plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "scores = []\n",
    "for i, index in enumerate(test_index):\n",
    "    image = tf.keras.preprocessing.image.load_img(\n",
    "        df_dataset.iloc[index].input_path,\n",
    "        color_mode='grayscale',\n",
    "        target_size=IMG_SHAPE)\n",
    "    truth = tf.keras.preprocessing.image.load_img(\n",
    "        df_dataset.iloc[index].truth_path,\n",
    "        color_mode='grayscale',\n",
    "        target_size=IMG_SHAPE)\n",
    "    truth = np.array(truth) / 255\n",
    "\n",
    "    image = np.atleast_3d(image)\n",
    "    image = np.array(image) / 255\n",
    "    pred = generator.predict(image[np.newaxis]).squeeze()\n",
    "    image = image.squeeze()\n",
    "    if i < 5:\n",
    "        show_images_in_a_row(*zip((image, 'input'), (pred, 'output'), (\n",
    "            truth, 'truth'), (image - pred, 'input-output'), (\n",
    "                image - truth, 'input-truth'), (pred - truth, 'output-truth')))\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator.save('unet_generator.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
