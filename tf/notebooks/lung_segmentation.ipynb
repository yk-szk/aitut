{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MYqrqzgd-t3j"
   },
   "source": [
    "# 肺野セグメンテーション\n",
    "参考 [Image segmentation  |  TensorFlow Core](https://www.tensorflow.org/tutorials/images/segmentation)\n",
    "\n",
    "## 前準備\n",
    "### 主要パッケージのインポート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WSFQMTTE-t3k"
   },
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "from logging import basicConfig, getLogger, INFO\n",
    "basicConfig(level=INFO, format='%(asctime)s %(levelname)s :%(message)s')\n",
    "logger = getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "moY2hgoO-t3o"
   },
   "source": [
    "### データディレクトリの指定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QTkR-zBp-t3p"
   },
   "outputs": [],
   "source": [
    "DATA_ROOT = pathlib.Path('Data/Images/chest_xray')\n",
    "IMAGE_DIR = 'regular'\n",
    "LABEL_DIR = 'lung'\n",
    "CLASS_LABELS = ('lung')\n",
    "IMAGE_EXT = '.png'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Pk8Hxqh_-t3s"
   },
   "source": [
    "### 画像ファイルを基にpd.DataFrameを作成する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "colab_type": "code",
    "id": "B0uZrUGE-t3s",
    "outputId": "c62f581b-e349-446b-95c2-d9a0503b08eb"
   },
   "outputs": [],
   "source": [
    "def create_dataset_df(data_root, image_dir, label_dir, image_ext):\n",
    "    dfs = []\n",
    "    root = pathlib.Path(data_root)\n",
    "    image_filenames = (root / pathlib.Path(image_dir)).glob('*' + image_ext)\n",
    "    df = pd.DataFrame(image_filenames, columns=['image_path'])\n",
    "    df['label_path'] = df['image_path'].map(\n",
    "        lambda p: root / pathlib.Path(label_dir) / p.name)\n",
    "    return df\n",
    "\n",
    "\n",
    "df_dataset = create_dataset_df(DATA_ROOT, IMAGE_DIR, LABEL_DIR, IMAGE_EXT)\n",
    "display(df_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MiL6rk0q-t3x"
   },
   "source": [
    "### 画像を表示\n",
    "入力画像を背景にセグメンテーションを重畳表示する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "id": "TrTnSTfN-t3y",
    "outputId": "3cdee2b5-203f-46de-f788-31bcaf617d1d"
   },
   "outputs": [],
   "source": [
    "OVERLAY_ALPHA = 0.5\n",
    "cmap = np.array([[0, 0, 0, 0], [255, 0, 0,\n",
    "                                255 * OVERLAY_ALPHA]]).astype(np.uint8)\n",
    "\n",
    "N_SAMPLES = 5\n",
    "plt.figure(figsize=(10, 5))\n",
    "for i, sample in enumerate(df_dataset.sample(n=N_SAMPLES).itertuples()):\n",
    "    image = Image.open(sample.image_path).convert('RGBA')\n",
    "    label = np.array(Image.open(sample.label_path).convert('L'))\n",
    "    label = (label > 0).astype(np.uint8)\n",
    "    label = Image.fromarray(cmap[label])\n",
    "    plt.subplot(1, N_SAMPLES, i + 1)\n",
    "    plt.imshow(Image.alpha_composite(image, label))\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ホールドアウト\n",
    "学習に時間がかかるため、今回は交差検証は行わない。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R0rthjmd-t31"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "N_SPLITS = 5\n",
    "kfold = KFold(n_splits=N_SPLITS, shuffle=True)\n",
    "train_index, test_index = next(kfold.split(df_dataset['image_path']))\n",
    "\n",
    "df_train = df_dataset.iloc[train_index]\n",
    "df_test = df_dataset.iloc[test_index]\n",
    "print('training:', len(df_train), 'test:', len(df_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8SIdvrDp-t37"
   },
   "source": [
    "## モデル作成\n",
    "MobileNetV2をベースにする。[参考](https://www.tensorflow.org/tutorials/images/segmentation \"参考\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KR1LGnLp-t37"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "# from https://github.com/tensorflow/examples/blob/master/tensorflow_examples/models/pix2pix/pix2pix.py\n",
    "def upsample(filters, size, norm_type='batchnorm', apply_dropout=False):\n",
    "    ''' Upsamples an input.\n",
    "\n",
    "    Conv2DTranspose => Batchnorm => Dropout => Relu\n",
    "\n",
    "    Args:\n",
    "      filters: number of filters\n",
    "      size: filter size\n",
    "      norm_type: Normalization type; either 'batchnorm' or 'instancenorm'.\n",
    "      apply_dropout: If True, adds the dropout layer\n",
    "\n",
    "    Returns:\n",
    "      Upsample Sequential Model\n",
    "    '''\n",
    "\n",
    "    initializer = tf.random_normal_initializer(0., 0.02)\n",
    "\n",
    "    result = tf.keras.Sequential()\n",
    "    result.add(\n",
    "        tf.keras.layers.Conv2DTranspose(filters,\n",
    "                                        size,\n",
    "                                        strides=2,\n",
    "                                        padding='same',\n",
    "                                        kernel_initializer=initializer,\n",
    "                                        use_bias=True))\n",
    "    result.add(tf.keras.layers.Activation('relu'))\n",
    "\n",
    "    result.add(\n",
    "        tf.keras.layers.Conv2D(filters,\n",
    "                               size,\n",
    "                               strides=1,\n",
    "                               padding='same',\n",
    "                               kernel_initializer=initializer,\n",
    "                               use_bias=False))\n",
    "    result.add(tf.keras.layers.Activation('relu'))\n",
    "    result.add(tf.keras.layers.BatchNormalization())\n",
    "    result.add(\n",
    "        tf.keras.layers.Conv2D(filters,\n",
    "                               size,\n",
    "                               strides=1,\n",
    "                               padding='same',\n",
    "                               kernel_initializer=initializer,\n",
    "                               use_bias=False))\n",
    "    result.add(tf.keras.layers.Activation('relu'))\n",
    "\n",
    "    if norm_type.lower() == 'batchnorm':\n",
    "        result.add(tf.keras.layers.BatchNormalization())\n",
    "    elif norm_type.lower() == 'instancenorm':\n",
    "        result.add(InstanceNormalization())\n",
    "\n",
    "    if apply_dropout:\n",
    "        result.add(tf.keras.layers.Dropout(0.5))\n",
    "\n",
    "    result.add(tf.keras.layers.ReLU())\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F5awhu6t-t3-"
   },
   "source": [
    "### Skip connectionとデコーダー\n",
    "- Encoder(MobileNetV2)にSkip connectionとデコーダーを追加する。\n",
    "- 入力サイズ512は大きすぎるのでAveragePooling2Dで小さくする。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "FzxQ59aM-t3-",
    "outputId": "131a37f2-bfae-4c63-bc46-bb47e85d4f3c"
   },
   "outputs": [],
   "source": [
    "def create_segmentation_model(input_shape):\n",
    "    base_model = tf.keras.applications.MobileNetV2(input_shape=[None, None, 3],\n",
    "                                                   include_top=False,\n",
    "                                                   weights=None)\n",
    "\n",
    "    base_weights = tf.keras.applications.MobileNetV2(\n",
    "        input_shape=[224, 224,\n",
    "                     3], include_top=False, weights='imagenet').get_weights()\n",
    "\n",
    "    base_model.set_weights(base_weights)\n",
    "\n",
    "    # Use the activations of these layers\n",
    "    layer_names = [\n",
    "        'block_1_expand_relu',  # 64x64\n",
    "        'block_3_expand_relu',  # 32x32\n",
    "        'block_6_expand_relu',  # 16x16\n",
    "        'block_13_expand_relu',  # 8x8\n",
    "        'block_16_project',  # 4x4\n",
    "    ]\n",
    "    layers = [base_model.get_layer(name).output for name in layer_names]\n",
    "\n",
    "    # Create the feature extraction model\n",
    "    down_stack = tf.keras.Model(inputs=base_model.input,\n",
    "                                outputs=layers,\n",
    "                                name='Encoder')\n",
    "\n",
    "    down_stack.trainable = False\n",
    "\n",
    "    up_stack = [\n",
    "        upsample(256, 3),  # 4x4 -> 8x8\n",
    "        upsample(128, 3),  # 8x8 -> 16x16\n",
    "        upsample(64, 3),  # 16x16 -> 32x32\n",
    "        upsample(32, 3),  # 32x32 -> 64x64\n",
    "    ]\n",
    "\n",
    "    inputs = tf.keras.layers.Input(shape=input_shape)\n",
    "    x = tf.keras.layers.AveragePooling2D(2)(inputs)  # down sampling\n",
    "\n",
    "    # Downsampling through the model\n",
    "    skips = down_stack(x)\n",
    "    x = skips[-1]\n",
    "    skips = reversed(skips[:-1])\n",
    "\n",
    "    # Upsampling and establishing the skip connections\n",
    "    for up, skip in zip(up_stack, skips):\n",
    "        x = up(x)\n",
    "        concat = tf.keras.layers.Concatenate()\n",
    "        x = concat([x, skip])\n",
    "\n",
    "    # This is the last layer of the model\n",
    "    x = tf.keras.layers.Conv2DTranspose(32,\n",
    "                                        3,\n",
    "                                        strides=2,\n",
    "                                        activation='relu',\n",
    "                                        padding='same')(x)  #64x64 -> 128x128\n",
    "\n",
    "    x = tf.keras.layers.Conv2D(16,\n",
    "                               3,\n",
    "                               strides=1,\n",
    "                               padding='same',\n",
    "                               activation='relu')(x)\n",
    "\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "\n",
    "    x = tf.keras.layers.Conv2DTranspose(8,\n",
    "                                        3,\n",
    "                                        strides=2,\n",
    "                                        activation='relu',\n",
    "                                        padding='same')(x)  #64x64 -> 128x128\n",
    "\n",
    "    x = tf.keras.layers.Conv2D(1, 3, strides=1, padding='same')(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=x)\n",
    "    return model\n",
    "\n",
    "\n",
    "model = create_segmentation_model((None, None, 3))\n",
    "tf.keras.utils.plot_model(model, show_shapes=True, dpi=70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m7Lh6SoE-t4D"
   },
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5m1qqUxc-t4F"
   },
   "source": [
    "## 学習\n",
    "### tf.data.Datasetを作成\n",
    "kerasのImageDataGeneratorがsemantic segmentationの場合使えなかったので、tensorflowのDatasetを使う。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "IMG_SHAPE = (512, 512, 3)\n",
    "\n",
    "\n",
    "def load_img(filepath):\n",
    "    return np.atleast_3d(\n",
    "        tf.keras.preprocessing.image.load_img(filepath, target_size=IMG_SHAPE))\n",
    "\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    np.stack([\n",
    "        load_img(filepath) for filepath in tqdm.tqdm(df_train['image_path'])\n",
    "    ]),\n",
    "    np.stack([\n",
    "        load_img(filepath)[..., :1]\n",
    "        for filepath in tqdm.tqdm(df_train['label_path'])\n",
    "    ]),\n",
    "))\n",
    "\n",
    "print(dataset.element_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data augmentation\n",
    "回転、左右反転等をランダムに適用する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 141
    },
    "colab_type": "code",
    "id": "0RxzGKGb-t4G",
    "outputId": "f60ff398-9fff-4267-99f0-3f19e1aa78e0"
   },
   "outputs": [],
   "source": [
    "import tensorflow_addons as tfa\n",
    "\n",
    "\n",
    "def convert(image, label):\n",
    "    image = tf.image.convert_image_dtype(image, tf.float32)\n",
    "    label = tf.image.convert_image_dtype(label, tf.float32)\n",
    "    label > 0\n",
    "    return image, label\n",
    "\n",
    "\n",
    "MAX_ANGLE_DEG = 10\n",
    "MAX_ANGLE_RAD = np.deg2rad(MAX_ANGLE_DEG)\n",
    "FLIP_RATE = .5\n",
    "BRIGHTNESS_RANGE = .5\n",
    "\n",
    "\n",
    "def augment(image, label):\n",
    "    image, label = convert(image, label)\n",
    "    # rotate\n",
    "    angle = tf.random.uniform((), minval=-MAX_ANGLE_RAD, maxval=MAX_ANGLE_RAD)\n",
    "    image = tfa.image.rotate(image, angle, interpolation='BILINEAR')\n",
    "    label = tfa.image.rotate(label, angle, interpolation='NEAREST')\n",
    "\n",
    "    # flip\n",
    "    if tf.random.uniform(()) < FLIP_RATE:\n",
    "        image = tf.image.flip_left_right(image)\n",
    "        label = tf.image.flip_left_right(label)\n",
    "\n",
    "    # brightness\n",
    "    image = image + tf.random.uniform(\n",
    "        (), minval=-BRIGHTNESS_RANGE, maxval=BRIGHTNESS_RANGE)\n",
    "\n",
    "    return image, label\n",
    "\n",
    "\n",
    "for d in dataset.map(augment):\n",
    "    plt.figure(figsize=(3, 2))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(d[0])\n",
    "    plt.axis('off')\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(d[1][..., 0])\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 395
    },
    "colab_type": "code",
    "id": "exhD5UZl-t4I",
    "outputId": "a8a49012-b07e-477c-d154-58119a4294d2"
   },
   "outputs": [],
   "source": [
    "import tut_utils\n",
    "\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "EPOCHS = 256\n",
    "PATIENCE = 16\n",
    "BATCH_SIZE = 4\n",
    "\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='loss',\n",
    "                                                  restore_best_weights=True,\n",
    "                                                  patience=PATIENCE,\n",
    "                                                  min_delta=0.002)\n",
    "\n",
    "logger.info('start of training')\n",
    "with tut_utils.ProgressBarCallback(EPOCHS,\n",
    "                         len(train_index) // BATCH_SIZE, BATCH_SIZE) as pbar:\n",
    "    result = model.fit(dataset.shuffle(BATCH_SIZE * 16).map(\n",
    "        augment,\n",
    "        num_parallel_calls=AUTOTUNE).batch(BATCH_SIZE).prefetch(AUTOTUNE),\n",
    "                       epochs=EPOCHS,\n",
    "                       verbose=0,\n",
    "                       shuffle=False,\n",
    "                       callbacks=[pbar, early_stopping])\n",
    "logger.info('end of training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o8OINUKZ-t4L"
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(result.history).plot(title='Training history', figsize=(5, 3))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "467GUQkS-t4O"
   },
   "source": [
    "## 評価\n",
    "Dice similarity coefficient(F1 score)とJaccard Index(IoU)を評価指標とする。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OXsNeBGr-t4O"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from sklearn import metrics\n",
    "\n",
    "label_cmap = np.array([[0, 0, 0], [255, 0, 0], [0, 255, 0], [255, 255, 0]])\n",
    "\n",
    "scores = []\n",
    "for i, index in enumerate(test_index):\n",
    "    image = tf.keras.preprocessing.image.load_img(\n",
    "        df_dataset.iloc[index].image_path, color_mode='rgb')\n",
    "    image = np.atleast_3d(image)\n",
    "    image = np.array(image) / 255\n",
    "    padding_size = max(512,\n",
    "                       math.ceil(image.shape[0] / (2**6)) *\n",
    "                       (2**6)) - image.shape[0]\n",
    "    padded_image = np.pad(image, ((0, padding_size), (0, 0), (0, 0)))\n",
    "\n",
    "    pred = tf.nn.sigmoid(model.predict(\n",
    "        padded_image[np.newaxis])).numpy().squeeze()\n",
    "    if padding_size > 0:\n",
    "        pred = pred[:-padding_size]\n",
    "\n",
    "    pred_bin = pred > .5\n",
    "    label = np.array(Image.open(df_dataset.iloc[index].label_path).convert('L')) > 0\n",
    "    scores.append((metrics.f1_score(label.ravel(), pred_bin.ravel()),\n",
    "                   metrics.jaccard_score(label.ravel(), pred_bin.ravel())))\n",
    "    if i < N_SAMPLES:\n",
    "        plt.subplot(1, 5, 1)\n",
    "        plt.imshow(image)\n",
    "        plt.axis('off')\n",
    "        plt.subplot(1, 5, 2)\n",
    "        plt.imshow(pred, cmap='jet')\n",
    "        plt.axis('off')\n",
    "        plt.subplot(1, 5, 3)\n",
    "        plt.imshow(label_cmap[[0, 1]][pred_bin.astype(np.uint8)])\n",
    "        plt.axis('off')\n",
    "        plt.subplot(1, 5, 4)\n",
    "        plt.imshow(label_cmap[[0, 2]][label.astype(np.uint8)])\n",
    "        plt.axis('off')\n",
    "        plt.subplot(1, 5, 5)\n",
    "        plt.imshow(label_cmap[pred_bin + label * 2])\n",
    "        plt.axis('off')\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jmPLf7yB-t4S"
   },
   "outputs": [],
   "source": [
    "df_score = pd.DataFrame(scores,\n",
    "                        columns=['dice coefficient', 'jaccard index'],\n",
    "                        index=test_index)\n",
    "display(df_score.head())\n",
    "display(\n",
    "    pd.DataFrame({\n",
    "        'median': df_score.median(),\n",
    "        'mean': df_score.mean(),\n",
    "        'std': df_score.std(),\n",
    "        'min': df_score.min(),\n",
    "        'max': df_score.max(),\n",
    "    }))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分布の確認\n",
    "Dice similarity coefficientの分布を表示する。\n",
    "\n",
    "#### Boxplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.boxplot(x='dice coefficient', data=df_score)\n",
    "sns.swarmplot(x='dice coefficient', data=df_score, color='.2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Letter value plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxenplot(x='dice coefficient', data=df_score)\n",
    "sns.swarmplot(x='dice coefficient',\n",
    "              data=df_score,\n",
    "              color='white',\n",
    "              edgecolor='black',\n",
    "              linewidth=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Violinplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.violinplot(x='dice coefficient', data=df_score, inner=None)\n",
    "sns.swarmplot(x='dice coefficient',\n",
    "              data=df_score,\n",
    "              color='white',\n",
    "              edgecolor='gray')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "lung_segmentation_tfdata.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "359.2px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
