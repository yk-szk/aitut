{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bone suppression(U-Net)\n",
    "\n",
    "## 前準備\n",
    "### 主要パッケージのインポート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "from logging import basicConfig, getLogger, INFO\n",
    "basicConfig(level=INFO, format='%(asctime)s %(levelname)s :%(message)s')\n",
    "logger = getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データディレクトリの指定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT = pathlib.Path('Data/Images/chest_xray')\n",
    "INPUT_IMAGE_DIR = 'bone_enhancement'\n",
    "TRUTH_IMAGE_DIR = 'bone_suppression'\n",
    "CLASS_LABELS = ('lung')\n",
    "IMAGE_EXT = '.png'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 画像ファイルを基にpd.DataFrameを作成する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset_df(data_root, input_dir, truth_dir, image_ext):\n",
    "    dfs = []\n",
    "    root = pathlib.Path(data_root)\n",
    "    image_filenames = (root / pathlib.Path(input_dir)).glob('*' + image_ext)\n",
    "    df = pd.DataFrame(image_filenames, columns=['input_path'])\n",
    "    df['truth_path'] = df['input_path'].map(\n",
    "        lambda p: root / pathlib.Path(truth_dir) / p.name)\n",
    "    return df\n",
    "\n",
    "\n",
    "df_dataset = create_dataset_df(DATA_ROOT, INPUT_IMAGE_DIR, TRUTH_IMAGE_DIR,\n",
    "                               IMAGE_EXT)\n",
    "display(df_dataset.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 画像を表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_SAMPLES = 3\n",
    "plt.figure(figsize=(10, 5))\n",
    "for i, sample in enumerate(df_dataset.sample(n=N_SAMPLES).itertuples()):\n",
    "    image = Image.open(sample.input_path)\n",
    "    truth = Image.open(sample.truth_path)\n",
    "    plt.figure(figsize=(4, 2))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(image, cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(truth, cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデル作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SHAPE = [1, 256, 256]\n",
    "OUTPUT_CHANNELS = 1\n",
    "UNET_DEPTH=7\n",
    "IN_CHS = 1\n",
    "OUT_CHS = 1\n",
    "INI_CHS = 16\n",
    "from tut_models import UNet\n",
    "from torchsummary import summary\n",
    "summary(UNet(IN_CHS, OUT_CHS, UNET_DEPTH, ini_chs=INI_CHS), INPUT_SHAPE, verbose=0, device='cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lit model\n",
    "平均二乗誤差を使う"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "\n",
    "NUM_WORKERS = 0 if os.name == 'nt' else 2\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "\n",
    "\n",
    "class LitUNet(pl.LightningModule):\n",
    "    def __init__(self, in_chs, out_chs, depth):\n",
    "        super().__init__()\n",
    "        self.model = UNet(in_chs, out_chs, depth, ini_chs=INI_CHS)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self.model(x)\n",
    "        loss = F.mse_loss(y_hat, y)\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer=torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=2)\n",
    "        return {'optimizer':optimizer, 'scheduler':scheduler, 'monitor':'train_loss'}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習\n",
    "### ホールドアウト"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "N_SPLITS = 5\n",
    "SEED = 0\n",
    "kfold = KFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
    "train_index, test_index = next(kfold.split(df_dataset['input_path']))\n",
    "\n",
    "df_train = df_dataset.iloc[train_index]\n",
    "df_test = df_dataset.iloc[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "IMG_SHAPE = INPUT_SHAPE\n",
    "\n",
    "\n",
    "def load_img(filepath):\n",
    "    img = Image.open(filepath)\n",
    "    img = img.resize(IMG_SHAPE[1:])\n",
    "    return np.atleast_3d(img)\n",
    "\n",
    "\n",
    "def load_segmentation_dataset(df, load_img):\n",
    "    data = np.stack(\n",
    "        [load_img(filepath) for filepath in tqdm.tqdm(df['input_path'])])\n",
    "    labels = np.stack([\n",
    "        load_img(filepath)[..., :1]\n",
    "        for filepath in tqdm.tqdm(df['truth_path'])\n",
    "    ])\n",
    "    return (data/255).astype(np.float32), (labels/255).astype(np.float32)\n",
    "\n",
    "train_data, train_labels = load_segmentation_dataset(df_train, load_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "\n",
    "album_transform = A.Compose([\n",
    "    A.RandomBrightnessContrast(brightness_limit=.1,\n",
    "                               contrast_limit=.1,\n",
    "                               p=.5),\n",
    "    A.HorizontalFlip(p=.25),\n",
    "    A.ShiftScaleRotate(shift_limit=0, scale_limit=.2, rotate_limit=10, p=.8)\n",
    "], additional_targets={'image0': 'image'})\n",
    "\n",
    "def random_transform(x, y):\n",
    "    tfed = album_transform(image=x, image0=y)\n",
    "    x, y = tfed['image'], tfed['image0']\n",
    "    x, y = x.transpose(2,0,1), y.transpose(2,0,1)\n",
    "    return x.astype(np.float32), y.astype(np.float32)\n",
    "\n",
    "\n",
    "class AugmentedDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, x, y, transform=None):\n",
    "        self.transform = transform\n",
    "        self.xs = x\n",
    "        self.ys = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.xs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x, y = self.xs[idx], self.ys[idx]\n",
    "        if self.transform:\n",
    "            x, y = self.transform(x, y)\n",
    "\n",
    "        return x, y\n",
    "\n",
    "N_TEST = 3\n",
    "\n",
    "\n",
    "def test_augmentation(df_dataset):\n",
    "    dataset = AugmentedDataset(train_data[:N_TEST], train_labels[:N_TEST], random_transform)\n",
    "\n",
    "    loader = torch.utils.data.DataLoader(dataset,\n",
    "                                         batch_size=1,\n",
    "                                         shuffle=False,\n",
    "                                         num_workers=0)\n",
    "    for i, data in enumerate(loader):\n",
    "        plt.figure(figsize=(4, 1.5))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(data[0].squeeze().numpy(), cmap='gray')\n",
    "        plt.title('input image')\n",
    "        plt.axis('off')\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(data[1].squeeze().numpy(), cmap='gray')\n",
    "        plt.title('true image')\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "test_augmentation(df_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 32\n",
    "PATIENCE = 4\n",
    "BATCH_SIZE = 4\n",
    "\n",
    "model = LitUNet(IN_CHS, OUT_CHS, UNET_DEPTH)\n",
    "early_stop_callback = EarlyStopping(monitor='train_loss',\n",
    "                                    patience=PATIENCE,\n",
    "                                    verbose=False,\n",
    "                                    mode='min')\n",
    "trainer = pl.Trainer(gpus=1 if torch.cuda.is_available() else 0,\n",
    "                     max_epochs=EPOCHS,\n",
    "                     callbacks=[early_stop_callback])\n",
    "\n",
    "dataset = AugmentedDataset(train_data, train_labels, random_transform)\n",
    "trainloader = torch.utils.data.DataLoader(dataset,\n",
    "                                          batch_size=BATCH_SIZE,\n",
    "                                          shuffle=True,\n",
    "                                          num_workers=NUM_WORKERS)\n",
    "trainer.fit(model, trainloader)\n",
    "logger.info('Finish training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 評価"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "def show_images_in_a_row(images, titles, figsize=(15, 5)):\n",
    "    assert len(images) == len(titles), 'Invalid size of arguments'\n",
    "    plt.figure(figsize=figsize)\n",
    "    for i, (image, title) in enumerate(zip(images, titles), 1):\n",
    "        plt.subplot(1, len(images), i)\n",
    "        plt.imshow(image, cmap='gray')\n",
    "        plt.title(title)\n",
    "        plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "model.eval()\n",
    "model.freeze()\n",
    "\n",
    "scores = []\n",
    "for i, index in enumerate(test_index):\n",
    "    image = load_img(\n",
    "        df_dataset.iloc[index].input_path)\n",
    "    truth = load_img(\n",
    "        df_dataset.iloc[index].truth_path)\n",
    "    truth = np.array(truth) / 255\n",
    "    truth = truth.squeeze()\n",
    "\n",
    "    image = np.atleast_3d(image)\n",
    "    image = np.array(image) / 255\n",
    "    image = image.transpose(2,0,1)\n",
    "    pred = model(torch.FloatTensor(image[np.newaxis])).squeeze().cpu().numpy()\n",
    "    image = image.squeeze()\n",
    "    if i < 5:\n",
    "        show_images_in_a_row(*zip((image, 'input'), (pred, 'output'), (\n",
    "            truth, 'truth'), (image - pred, 'input-output'), (\n",
    "                image - truth, 'input-truth'), (pred - truth, 'output-truth')))\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_checkpoint('unet_generator.ckpt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "316.475px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
