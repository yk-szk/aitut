{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bone suppression(pix2pix)\n",
    "\n",
    "\n",
    "## 前準備\n",
    "### 主要パッケージのインポート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "from logging import basicConfig, getLogger, INFO\n",
    "basicConfig(level=INFO, format='%(asctime)s %(levelname)s :%(message)s')\n",
    "logger = getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データディレクトリの指定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT = pathlib.Path('Data/Images/chest_xray')\n",
    "INPUT_IMAGE_DIR = 'bone_enhancement'\n",
    "TRUTH_IMAGE_DIR = 'bone_suppression'\n",
    "CLASS_LABELS = ('lung')\n",
    "IMAGE_EXT = '.png'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 画像ファイルを基にpd.DataFrameを作成する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset_df(data_root, input_dir, truth_dir, image_ext):\n",
    "    dfs = []\n",
    "    root = pathlib.Path(data_root)\n",
    "    image_filenames = (root / pathlib.Path(input_dir)).glob('*' + image_ext)\n",
    "    df = pd.DataFrame(image_filenames, columns=['input_path'])\n",
    "    df['truth_path'] = df['input_path'].map(\n",
    "        lambda p: root / pathlib.Path(truth_dir) / p.name)\n",
    "    return df\n",
    "\n",
    "\n",
    "df_dataset = create_dataset_df(DATA_ROOT, INPUT_IMAGE_DIR, TRUTH_IMAGE_DIR,\n",
    "                               IMAGE_EXT)\n",
    "display(df_dataset.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 画像を表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_SAMPLES = 3\n",
    "plt.figure(figsize=(10, 5))\n",
    "for i, sample in enumerate(df_dataset.sample(n=N_SAMPLES).itertuples()):\n",
    "    image = Image.open(sample.input_path)\n",
    "    truth = Image.open(sample.truth_path)\n",
    "    plt.figure(figsize=(4, 2))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(image, cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(truth, cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import tut_utils\n",
    "INPUT_SHAPE = [1, 256, 256]\n",
    "IMG_SHAPE = INPUT_SHAPE\n",
    "\n",
    "\n",
    "def load_img(filepath):\n",
    "    img = Image.open(filepath)\n",
    "    img = img.resize(IMG_SHAPE[1:])\n",
    "    return np.atleast_3d(img)\n",
    "\n",
    "\n",
    "load_img2img_dataset = functools.partial(tut_utils.load_img2img_dataset,\n",
    "                                         input_column='input_path',\n",
    "                                         target_column='truth_path')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "from tut_utils import AugmentedDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "album_transform = A.Compose([\n",
    "    A.RandomBrightnessContrast(brightness_limit=.1, contrast_limit=.1, p=.5),\n",
    "    A.HorizontalFlip(p=.25),\n",
    "    A.ShiftScaleRotate(shift_limit=0, scale_limit=.2, rotate_limit=10, p=.8)\n",
    "],\n",
    "                            additional_targets={'image0': 'image'})\n",
    "\n",
    "\n",
    "def random_transform(x, y):\n",
    "    x = (x / 255).astype(np.float32)\n",
    "    y = (y / 255).astype(np.float32)\n",
    "    tfed = album_transform(image=x, image0=y)\n",
    "    x, y = tfed['image'], tfed['image0']\n",
    "    x, y = x.transpose(2, 0, 1), y.transpose(2, 0, 1)\n",
    "    return x.astype(np.float32), y.astype(np.float32)\n",
    "\n",
    "\n",
    "N_TEST = 3\n",
    "\n",
    "\n",
    "def test_augmentation(df_dataset):\n",
    "    df_train = df_dataset.iloc[:N_TEST]\n",
    "    train_data, train_labels = load_img2img_dataset(df_train, load_img)\n",
    "    dataset = AugmentedDataset(train_data, train_labels, random_transform)\n",
    "\n",
    "    loader = DataLoader(dataset, batch_size=1, shuffle=False, num_workers=0)\n",
    "    for i, data in enumerate(loader):\n",
    "        plt.figure(figsize=(4, 1.5))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(data[0].squeeze().numpy(), cmap='gray')\n",
    "        plt.title('input image')\n",
    "        plt.axis('off')\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(data[1].squeeze().numpy(), cmap='gray')\n",
    "        plt.title('target image')\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "test_augmentation(df_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデル作成\n",
    "\n",
    "### Generator model\n",
    "UNet ver.と同じものを使う"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_CHANNELS = 1\n",
    "UNET_DEPTH = 6\n",
    "IN_CHS = 1\n",
    "OUT_CHS = 1\n",
    "INI_CHS = 16\n",
    "from tut_models import UNet\n",
    "from torchsummary import summary\n",
    "summary(UNet(IN_CHS, OUT_CHS, UNET_DEPTH, ini_chs=INI_CHS),\n",
    "        INPUT_SHAPE,\n",
    "        verbose=0,\n",
    "        device='cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminator model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import tut_models\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            tut_models.ConvBNReLU(2, 16, 3, 0, leaky=True),\n",
    "            tut_models.ConvBNReLU(16, 32, 3, 0, leaky=True), nn.MaxPool2d(2),\n",
    "            tut_models.ConvBNReLU(32, 32, 3, 0, leaky=True),\n",
    "            tut_models.ConvBNReLU(32, 64, 3, 0, leaky=True), nn.MaxPool2d(2),\n",
    "            tut_models.ConvBNReLU(64, 64, 3, 0, leaky=True),\n",
    "            tut_models.ConvBNReLU(64, 128, 3, 0, leaky=True), nn.MaxPool2d(2),\n",
    "            tut_models.ConvBNReLU(128, 128, 3, 0, leaky=True),\n",
    "            tut_models.ConvBNReLU(128, 256, 3, 0, leaky=True), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(256, 1, kernel_size=3, padding=0))\n",
    "\n",
    "    def forward(self, input_image, target_image):\n",
    "        cat = torch.cat([input_image, target_image], dim=1)\n",
    "        return self.model(cat)\n",
    "\n",
    "\n",
    "summary(Discriminator(), [INPUT_SHAPE, INPUT_SHAPE], verbose=0, device='cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習\n",
    "### ホールドアウト"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "N_SPLITS = 5\n",
    "SEED = 0\n",
    "kfold = KFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
    "train_index, test_index = next(kfold.split(df_dataset['input_path']))\n",
    "\n",
    "df_train = df_dataset.iloc[train_index]\n",
    "df_test = df_dataset.iloc[test_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class LitPix2Pix(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.generator = UNet(IN_CHS, OUT_CHS, UNET_DEPTH, ini_chs=INI_CHS)\n",
    "        self.discriminator = Discriminator()\n",
    "        self.pix_weight = 100  # weight for pixelwise loss\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.generator(x)\n",
    "\n",
    "    def adversarial_loss(self, y_hat, y):\n",
    "        return F.binary_cross_entropy_with_logits(y_hat, y)\n",
    "\n",
    "    def pixelwise_loss(self, input_data, target_data):\n",
    "        return F.l1_loss(input_data, target_data)\n",
    "\n",
    "    def training_step(self, batch, batch_idx, optimizer_idx):\n",
    "        input_imgs, target_imgs = batch\n",
    "        self.input, self.target = input_imgs, target_imgs\n",
    "\n",
    "        if optimizer_idx == 0:  # train generator\n",
    "            g_out = self(input_imgs)\n",
    "            self.g_out = g_out\n",
    "\n",
    "            d_out = self.discriminator(input_imgs, self.g_out)\n",
    "\n",
    "            # adversarial loss\n",
    "            g_loss_adv = self.adversarial_loss(d_out, torch.ones_like(d_out))\n",
    "            # pixelwise loss\n",
    "            g_loss_pix = self.pixelwise_loss(g_out, target_imgs)\n",
    "            g_loss = g_loss_adv + (self.pix_weight * g_loss_pix)\n",
    "            loss = g_loss\n",
    "\n",
    "        if optimizer_idx == 1:  # train discriminator\n",
    "            self.g_out.detach()\n",
    "\n",
    "            # target_imgs is real\n",
    "            d_out = self.discriminator(input_imgs, target_imgs)\n",
    "            d_loss_real = self.adversarial_loss(d_out, torch.ones_like(d_out))\n",
    "\n",
    "            # g_out is fake\n",
    "            d_out = self.discriminator(input_imgs, self.g_out)\n",
    "            d_loss_fake = self.adversarial_loss(d_out, torch.zeros_like(d_out))\n",
    "\n",
    "            # discriminator loss is the average of these\n",
    "            d_loss = .5 * (d_loss_real + d_loss_fake)\n",
    "            loss = d_loss\n",
    "\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        opt_g = optim.Adam(self.generator.parameters(), lr=2e-4)\n",
    "        opt_d = optim.Adam(self.discriminator.parameters(), lr=2e-4)\n",
    "        return [opt_g, opt_d], []\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.current_epoch % 4 != 0:\n",
    "            return\n",
    "        for i, img in enumerate([self.input[0], self.target[0],\n",
    "                                 self.g_out[0]]):\n",
    "            img = img.cpu().numpy().squeeze()\n",
    "            plt.subplot(1, 3, i + 1)\n",
    "            plt.imshow(img, cmap='gray')\n",
    "            plt.axis('off')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "EPOCHS = 32\n",
    "BATCH_SIZE = 2\n",
    "NUM_WORKERS = 0 if os.name == 'nt' else 2\n",
    "gpus = 1 if torch.cuda.is_available() else 0\n",
    "# gpus = 0\n",
    "\n",
    "train_data, train_labels = load_img2img_dataset(df_train,\n",
    "                                                load_img,\n",
    "                                                verbose=True)\n",
    "dataset = AugmentedDataset(train_data, train_labels, random_transform)\n",
    "trainloader = DataLoader(dataset,\n",
    "                         batch_size=BATCH_SIZE,\n",
    "                         shuffle=True,\n",
    "                         num_workers=NUM_WORKERS)\n",
    "\n",
    "model = LitPix2Pix()\n",
    "trainer = pl.Trainer(gpus=gpus, max_epochs=EPOCHS)\n",
    "\n",
    "trainer.fit(model, trainloader)\n",
    "logger.info('Finish training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 評価\n",
    "目視で評価する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "def show_images_in_a_row(images, titles, figsize=(15, 5)):\n",
    "    assert len(images) == len(titles), 'Invalid size of arguments'\n",
    "    plt.figure(figsize=figsize)\n",
    "    for i, (image, title) in enumerate(zip(images, titles), 1):\n",
    "        plt.subplot(1, len(images), i)\n",
    "        plt.imshow(image, cmap='gray')\n",
    "        plt.title(title)\n",
    "        plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "model.eval()\n",
    "model.freeze()\n",
    "\n",
    "scores = []\n",
    "for i, index in enumerate(test_index):\n",
    "    image = load_img(df_dataset.iloc[index].input_path)\n",
    "    truth = load_img(df_dataset.iloc[index].truth_path)\n",
    "    truth = np.array(truth) / 255\n",
    "    truth = truth.squeeze()\n",
    "\n",
    "    image = np.atleast_3d(image)\n",
    "    image = np.array(image) / 255\n",
    "    image = image.transpose(2, 0, 1)\n",
    "    pred = model(torch.FloatTensor(image[np.newaxis])).squeeze().cpu().numpy()\n",
    "    image = image.squeeze()\n",
    "    if i < 5:\n",
    "        show_images_in_a_row(*zip((image, 'input'), (pred, 'output'), (\n",
    "            truth, 'truth'), (image - pred, 'input-output'), (\n",
    "                image - truth, 'input-truth'), (pred - truth, 'output-truth')))\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_checkpoint('pix2pix_generator.ckpt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "262.875px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "278.667px",
    "left": "2214.33px",
    "right": "20px",
    "top": "120px",
    "width": "325.667px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
