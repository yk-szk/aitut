{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bone suppression(pix2pix)\n",
    "\n",
    "\n",
    "## 前準備\n",
    "### 主要パッケージのインポート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "from logging import basicConfig, getLogger, INFO\n",
    "basicConfig(level=INFO, format='%(asctime)s %(levelname)s :%(message)s')\n",
    "logger = getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データディレクトリの指定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT = pathlib.Path('Data/Images/chest_xray')\n",
    "INPUT_IMAGE_DIR = 'bone_enhancement'\n",
    "TRUTH_IMAGE_DIR = 'bone_suppression'\n",
    "CLASS_LABELS = ('lung')\n",
    "IMAGE_EXT = '.png'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 画像ファイルを基にpd.DataFrameを作成する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset_df(data_root, input_dir, truth_dir, image_ext):\n",
    "    dfs = []\n",
    "    root = pathlib.Path(data_root)\n",
    "    image_filenames = (root / pathlib.Path(input_dir)).glob('*' + image_ext)\n",
    "    df = pd.DataFrame(image_filenames, columns=['input_path'])\n",
    "    df['truth_path'] = df['input_path'].map(\n",
    "        lambda p: root / pathlib.Path(truth_dir) / p.name)\n",
    "    return df\n",
    "\n",
    "\n",
    "df_dataset = create_dataset_df(DATA_ROOT, INPUT_IMAGE_DIR, TRUTH_IMAGE_DIR,\n",
    "                               IMAGE_EXT)\n",
    "display(df_dataset.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 画像を表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_SAMPLES = 3\n",
    "plt.figure(figsize=(10, 5))\n",
    "for i, sample in enumerate(df_dataset.sample(n=N_SAMPLES).itertuples()):\n",
    "    image = Image.open(sample.input_path)\n",
    "    truth = Image.open(sample.truth_path)\n",
    "    plt.figure(figsize=(4, 2))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(image, cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(truth, cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデル作成\n",
    "\n",
    "### Generator model\n",
    "UNet ver.と同じものを使う"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SHAPE = [1, 256, 256]\n",
    "OUTPUT_CHANNELS = 1\n",
    "UNET_DEPTH=7\n",
    "IN_CHS = 1\n",
    "OUT_CHS = 1\n",
    "INI_CHS = 16\n",
    "from tut_models import UNet\n",
    "from torchsummary import summary\n",
    "summary(UNet(IN_CHS, OUT_CHS, UNET_DEPTH, ini_chs=INI_CHS), INPUT_SHAPE, verbose=0, device='cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminator model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import tut_models\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            tut_models.ConvBNReLU(2, 16, 3, 0, leaky=True),\n",
    "            tut_models.ConvBNReLU(16, 32, 3, 0, leaky=True),\n",
    "            nn.MaxPool2d(2),\n",
    "            tut_models.ConvBNReLU(32, 32, 3, 0, leaky=True),\n",
    "            tut_models.ConvBNReLU(32, 64, 3, 0, leaky=True),\n",
    "            nn.MaxPool2d(2),\n",
    "            tut_models.ConvBNReLU(64, 64, 3, 0, leaky=True),\n",
    "            tut_models.ConvBNReLU(64, 128, 3, 0, leaky=True),\n",
    "            nn.MaxPool2d(2),\n",
    "            tut_models.ConvBNReLU(128, 128, 3, 0, leaky=True),\n",
    "            tut_models.ConvBNReLU(128, 256, 3, 0, leaky=True),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(256, 1, kernel_size=3, padding=0)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_image, target_image):\n",
    "        cat = torch.cat([input_image, target_image], dim=1)\n",
    "        return self.model(cat)\n",
    "\n",
    "\n",
    "summary(Discriminator(), [INPUT_SHAPE, INPUT_SHAPE], verbose=0, device='cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習\n",
    "### ホールドアウト"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "N_SPLITS = 5\n",
    "SEED = 0\n",
    "kfold = KFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
    "train_index, test_index = next(kfold.split(df_dataset['input_path']))\n",
    "\n",
    "df_train = df_dataset.iloc[train_index]\n",
    "df_test = df_dataset.iloc[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "IMG_SHAPE = INPUT_SHAPE\n",
    "\n",
    "\n",
    "def load_img(filepath):\n",
    "    img = Image.open(filepath)\n",
    "    img = img.resize(IMG_SHAPE[1:])\n",
    "    return np.atleast_3d(img)\n",
    "\n",
    "\n",
    "def load_segmentation_dataset(df, load_img):\n",
    "    data = np.stack(\n",
    "        [load_img(filepath) for filepath in tqdm.tqdm(df['input_path'])])\n",
    "    labels = np.stack([\n",
    "        load_img(filepath)[..., :1]\n",
    "        for filepath in tqdm.tqdm(df['truth_path'])\n",
    "    ])\n",
    "    return (data/255).astype(np.float32), (labels/255).astype(np.float32)\n",
    "\n",
    "train_data, train_labels = load_segmentation_dataset(df_train, load_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "\n",
    "album_transform = A.Compose([\n",
    "    A.RandomBrightnessContrast(brightness_limit=.1,\n",
    "                               contrast_limit=.1,\n",
    "                               p=.5),\n",
    "    A.HorizontalFlip(p=.25),\n",
    "    A.ShiftScaleRotate(shift_limit=0, scale_limit=.2, rotate_limit=10, p=.8)\n",
    "], additional_targets={'image0': 'image'})\n",
    "\n",
    "def random_transform(x, y):\n",
    "    tfed = album_transform(image=x, image0=y)\n",
    "    x, y = tfed['image'], tfed['image0']\n",
    "    x, y = x.transpose(2,0,1), y.transpose(2,0,1)\n",
    "    return x.astype(np.float32), y.astype(np.float32)\n",
    "\n",
    "\n",
    "class AugmentedDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, x, y, transform=None):\n",
    "        self.transform = transform\n",
    "        self.xs = x\n",
    "        self.ys = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.xs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x, y = self.xs[idx], self.ys[idx]\n",
    "        if self.transform:\n",
    "            x, y = self.transform(x, y)\n",
    "\n",
    "        return x, y\n",
    "\n",
    "N_TEST = 3\n",
    "\n",
    "\n",
    "def test_augmentation(df_dataset):\n",
    "    dataset = AugmentedDataset(train_data[:N_TEST], train_labels[:N_TEST], random_transform)\n",
    "\n",
    "    loader = torch.utils.data.DataLoader(dataset,\n",
    "                                         batch_size=1,\n",
    "                                         shuffle=False,\n",
    "                                         num_workers=0)\n",
    "    for i, data in enumerate(loader):\n",
    "        plt.figure(figsize=(4, 1.5))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(data[0].squeeze().numpy(), cmap='gray')\n",
    "        plt.title('input image')\n",
    "        plt.axis('off')\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(data[1].squeeze().numpy(), cmap='gray')\n",
    "        plt.title('true image')\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "test_augmentation(df_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from collections import OrderedDict\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class LitPix2Pix(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.generator = UNet(IN_CHS, OUT_CHS, UNET_DEPTH, ini_chs=INI_CHS)\n",
    "        self.discriminator = Discriminator()\n",
    "        self.pix_weight = 100\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.generator(x)\n",
    "\n",
    "    def adversarial_loss(self, y_hat, y):\n",
    "        return F.binary_cross_entropy_with_logits(y_hat, y)\n",
    "    \n",
    "    def pixelwise_loss(self, input_data, target_data):\n",
    "        return F.l1_loss(input_data, target_data)\n",
    "\n",
    "    def training_step(self, batch, batch_idx, optimizer_idx):\n",
    "        input_imgs, target_imgs = batch\n",
    "        self.input = input_imgs\n",
    "        self.target = target_imgs\n",
    "\n",
    "        g_out = self(input_imgs)\n",
    "        self.g_out = g_out\n",
    "        # train generator\n",
    "        if optimizer_idx == 0:\n",
    "\n",
    "            d_out = self.discriminator(input_imgs, self.g_out)\n",
    "\n",
    "            # adversarial loss is binary cross-entropy\n",
    "            g_loss_adv = self.adversarial_loss(d_out, torch.ones_like(d_out))\n",
    "            g_loss_pix = self.pixelwise_loss(g_out, target_imgs)\n",
    "            g_loss = g_loss_adv + (self.pix_weight * g_loss_pix)\n",
    "            tqdm_dict = {'g_loss': g_loss}\n",
    "            output = OrderedDict({\n",
    "                'loss': g_loss,\n",
    "                'progress_bar': tqdm_dict,\n",
    "                'log': tqdm_dict\n",
    "            })\n",
    "            return output\n",
    "\n",
    "        # train discriminator\n",
    "        if optimizer_idx == 1:\n",
    "            g_out.detach()\n",
    "\n",
    "            # target_imgs is real\n",
    "            d_out = self.discriminator(input_imgs, target_imgs)\n",
    "            d_loss_real = self.adversarial_loss(d_out, torch.ones_like(d_out))\n",
    "\n",
    "            # g_out is fake\n",
    "            d_out = self.discriminator(input_imgs, g_out)\n",
    "            d_loss_fake = self.adversarial_loss(d_out, torch.zeros_like(d_out))\n",
    "\n",
    "            # discriminator loss is the average of these\n",
    "            d_loss = .5 * (d_loss_real + d_loss_fake)\n",
    "            tqdm_dict = {'d_loss': d_loss}\n",
    "            output = OrderedDict({\n",
    "                'loss': d_loss,\n",
    "                'progress_bar': tqdm_dict,\n",
    "                'log': tqdm_dict\n",
    "            })\n",
    "            return output\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        opt_g = torch.optim.Adam(self.generator.parameters(),\n",
    "                                 lr=2e-4)\n",
    "        opt_d = torch.optim.Adam(self.discriminator.parameters(),\n",
    "                                 lr=2e-4)\n",
    "        return [opt_g, opt_d], []\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        for i, img in enumerate([self.input[0], self.target[0], self.g_out[0]]):\n",
    "            img = img.cpu().numpy().squeeze()\n",
    "            plt.subplot(1,3,i+1)\n",
    "            plt.imshow(img, cmap='gray')\n",
    "            plt.axis('off')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "EPOCHS = 16\n",
    "BATCH_SIZE = 4\n",
    "NUM_WORKERS = 0 if os.name == 'nt' else 2\n",
    "gpus = 1 if torch.cuda.is_available() else 0\n",
    "# gpus = 0\n",
    "\n",
    "model = LitPix2Pix()\n",
    "trainer = pl.Trainer(gpus=gpus,\n",
    "                     max_epochs=EPOCHS)\n",
    "\n",
    "dataset = AugmentedDataset(train_data, train_labels, random_transform)\n",
    "trainloader = torch.utils.data.DataLoader(dataset,\n",
    "                                          batch_size=BATCH_SIZE,\n",
    "                                          shuffle=True,\n",
    "                                          num_workers=NUM_WORKERS)\n",
    "trainer.fit(model, trainloader)\n",
    "logger.info('Finish training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 評価\n",
    "目視"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "def show_images_in_a_row(images, titles, figsize=(15, 5)):\n",
    "    assert len(images) == len(titles), 'Invalid size of arguments'\n",
    "    plt.figure(figsize=figsize)\n",
    "    for i, (image, title) in enumerate(zip(images, titles), 1):\n",
    "        plt.subplot(1, len(images), i)\n",
    "        plt.imshow(image, cmap='gray')\n",
    "        plt.title(title)\n",
    "        plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "model.eval()\n",
    "model.freeze()\n",
    "\n",
    "scores = []\n",
    "for i, index in enumerate(test_index):\n",
    "    image = load_img(\n",
    "        df_dataset.iloc[index].input_path)\n",
    "    truth = load_img(\n",
    "        df_dataset.iloc[index].truth_path)\n",
    "    truth = np.array(truth) / 255\n",
    "    truth = truth.squeeze()\n",
    "\n",
    "    image = np.atleast_3d(image)\n",
    "    image = np.array(image) / 255\n",
    "    image = image.transpose(2,0,1)\n",
    "    pred = model(torch.FloatTensor(image[np.newaxis])).squeeze().cpu().numpy()\n",
    "    image = image.squeeze()\n",
    "    if i < 5:\n",
    "        show_images_in_a_row(*zip((image, 'input'), (pred, 'output'), (\n",
    "            truth, 'truth'), (image - pred, 'input-output'), (\n",
    "                image - truth, 'input-truth'), (pred - truth, 'output-truth')))\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_checkpoint('pix2pix_generator.ckpt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "262.875px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "278.667px",
    "left": "2214.33px",
    "right": "20px",
    "top": "120px",
    "width": "325.667px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
