{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# びまん性肺疾患(4クラス, (train, validate, test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 前準備\n",
    "### 主要パッケージを読み込む\n",
    "loggerの設定も行う"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-24T17:00:32.626947Z",
     "iopub.status.busy": "2020-12-24T17:00:32.625946Z",
     "iopub.status.idle": "2020-12-24T17:00:33.235207Z",
     "shell.execute_reply": "2020-12-24T17:00:33.234208Z"
    }
   },
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "from logging import basicConfig, getLogger, INFO\n",
    "basicConfig(level=INFO, format='%(asctime)s %(levelname)s :%(message)s')\n",
    "logger = getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データディレクトリの指定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-24T17:00:33.238194Z",
     "iopub.status.busy": "2020-12-24T17:00:33.238194Z",
     "iopub.status.idle": "2020-12-24T17:00:33.240192Z",
     "shell.execute_reply": "2020-12-24T17:00:33.240192Z"
    }
   },
   "outputs": [],
   "source": [
    "DATA_ROOT = pathlib.Path('Data/Images/LIDC_DLD')\n",
    "CLASS_LABELS = ('normal', 'GGO', 'emphysema', 'honeycomb')\n",
    "IMAGE_EXT = '.png'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 画像ファイルを基にpd.DataFrameを作成する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-24T17:00:33.243198Z",
     "iopub.status.busy": "2020-12-24T17:00:33.243198Z",
     "iopub.status.idle": "2020-12-24T17:00:34.340434Z",
     "shell.execute_reply": "2020-12-24T17:00:34.340434Z"
    }
   },
   "outputs": [],
   "source": [
    "import tut_utils\n",
    "df_dataset = tut_utils.create_dataset_df(DATA_ROOT, CLASS_LABELS, IMAGE_EXT)\n",
    "assert set(CLASS_LABELS) == set(df_dataset['class_label'].unique(\n",
    ")), 'Discrepancy between CLASS_LABELS and df_dataset'\n",
    "display(df_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### クラス毎の画像数を確認する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-24T17:00:34.346427Z",
     "iopub.status.busy": "2020-12-24T17:00:34.345428Z",
     "iopub.status.idle": "2020-12-24T17:00:34.349424Z",
     "shell.execute_reply": "2020-12-24T17:00:34.349424Z"
    }
   },
   "outputs": [],
   "source": [
    "df_dataset['class_label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 各クラスの画像を表示してみる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-24T17:00:34.367406Z",
     "iopub.status.busy": "2020-12-24T17:00:34.352421Z",
     "iopub.status.idle": "2020-12-24T17:00:35.035357Z",
     "shell.execute_reply": "2020-12-24T17:00:35.034358Z"
    }
   },
   "outputs": [],
   "source": [
    "import tut_utils\n",
    "tut_utils.show_images_each_class(df_dataset, n_rows=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データ読み込み用の関数を作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-24T17:00:35.039353Z",
     "iopub.status.busy": "2020-12-24T17:00:35.038359Z",
     "iopub.status.idle": "2020-12-24T17:00:35.042349Z",
     "shell.execute_reply": "2020-12-24T17:00:35.041351Z"
    }
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "IMG_SHAPE = (1, 32, 32)\n",
    "\n",
    "\n",
    "def load_img(filepath):\n",
    "    img = Image.open(filepath)\n",
    "    img = img.resize(IMG_SHAPE[1:])\n",
    "    return np.atleast_3d(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data augmentation\n",
    "いくつかの画像に対して実際にaugmentationを適用し表示する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-24T17:00:35.049342Z",
     "iopub.status.busy": "2020-12-24T17:00:35.049342Z",
     "iopub.status.idle": "2020-12-24T17:00:35.607010Z",
     "shell.execute_reply": "2020-12-24T17:00:35.607010Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import albumentations as A\n",
    "from tut_utils import AugmentedDataset, load_dataset\n",
    "\n",
    "album_transform = A.Compose([\n",
    "    A.RandomBrightnessContrast(brightness_limit=.1, contrast_limit=.1, p=.5),\n",
    "    A.Flip(p=.5),\n",
    "    A.ShiftScaleRotate(shift_limit=0.05,\n",
    "                       scale_limit=.1,\n",
    "                       rotate_limit=180,\n",
    "                       p=.8)\n",
    "])\n",
    "\n",
    "N_TEST = 3\n",
    "\n",
    "\n",
    "def random_transform(x, y):\n",
    "    x = (x / 255).astype(np.float32)\n",
    "    tfed = album_transform(image=x)\n",
    "    x = tfed['image']\n",
    "    x = x.transpose(2, 0, 1)  # to channels first\n",
    "    return x.astype(np.float32), y\n",
    "\n",
    "\n",
    "def base_transform(x, y):\n",
    "    x = x / 255\n",
    "    x = x.transpose(2, 0, 1)  # to channels first\n",
    "    return x.astype(np.float32), y\n",
    "\n",
    "\n",
    "def test_augmentation(df_dataset):\n",
    "    df_train = df_dataset.iloc[:N_TEST]\n",
    "    train_data, train_labels = load_dataset(df_train, load_img)\n",
    "    dataset = AugmentedDataset(train_data, train_labels, random_transform)\n",
    "    loader = torch.utils.data.DataLoader(dataset,\n",
    "                                         batch_size=1,\n",
    "                                         shuffle=False,\n",
    "                                         num_workers=0)\n",
    "    for i, data in enumerate(loader):\n",
    "        plt.figure(figsize=(4, 1.5))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(train_data[i], cmap='gray')\n",
    "        plt.title('pre-augmentation')\n",
    "        plt.axis('off')\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(data[0].squeeze().numpy(), cmap='gray')\n",
    "        plt.title('post-augmentation')\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "test_augmentation(df_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ネットワーク作成\n",
    "画像が小さいのでこれまでより小さいネットワークを作成する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-24T17:00:35.614007Z",
     "iopub.status.busy": "2020-12-24T17:00:35.614007Z",
     "iopub.status.idle": "2020-12-24T17:00:35.638973Z",
     "shell.execute_reply": "2020-12-24T17:00:35.639963Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_chs, out_chs, kernel_size=3):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(in_chs, out_chs, kernel_size=kernel_size),\n",
    "            nn.BatchNorm2d(out_chs), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_chs, out_chs, kernel_size=kernel_size),\n",
    "            nn.BatchNorm2d(out_chs), nn.ReLU(inplace=True))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(ConvBlock(1, 8), nn.MaxPool2d(2),\n",
    "                                     ConvBlock(8, 16), nn.MaxPool2d(2),\n",
    "                                     nn.Dropout(.25), nn.Flatten(start_dim=1),\n",
    "                                     nn.Linear(400, 32), nn.ReLU(inplace=True),\n",
    "                                     nn.Linear(32, len(CLASS_LABELS)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "\n",
    "from torchsummary import summary\n",
    "summary(CNN(), IMG_SHAPE, verbose=False, device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-24T17:00:35.644956Z",
     "iopub.status.busy": "2020-12-24T17:00:35.644956Z",
     "iopub.status.idle": "2020-12-24T17:00:35.845751Z",
     "shell.execute_reply": "2020-12-24T17:00:35.845751Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "import os\n",
    "\n",
    "\n",
    "class LitNet(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = CNN()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def shared_step(self, batch):\n",
    "        x, y = batch\n",
    "        y_hat = self.model(x)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss = self.shared_step(batch)\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss = self.shared_step(batch)\n",
    "        self.log('val_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Fold 交差検証(train, validate, test)\n",
    "各fold中でデータセットの$\\frac{2}{4}$を学習用、$\\frac{1}{4}$をvalidation(EarlyStopping)用、$\\frac{1}{4}$を評価用に使用する\n",
    "\n",
    "### DataFrameに交差検証用の列を追加する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-24T17:00:35.849757Z",
     "iopub.status.busy": "2020-12-24T17:00:35.849757Z",
     "iopub.status.idle": "2020-12-24T17:00:35.868753Z",
     "shell.execute_reply": "2020-12-24T17:00:35.868753Z"
    }
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "K_FOLD = 4\n",
    "kfold = StratifiedKFold(n_splits=K_FOLD, shuffle=True)\n",
    "\n",
    "test_indices = [\n",
    "    t[1] for t in kfold.split(df_dataset['filepath'], df_dataset['class'])\n",
    "]\n",
    "index2fold = dict(\n",
    "    list(\n",
    "        itertools.chain(*[[(idx, i) for idx in indices]\n",
    "                          for i, indices in enumerate(test_indices)])))\n",
    "\n",
    "df_dataset['set'] = df_dataset.index.map(index2fold)\n",
    "df_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train, validate, testを用いた交差検証を行う"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-24T17:00:35.904569Z",
     "iopub.status.busy": "2020-12-24T17:00:35.904569Z",
     "iopub.status.idle": "2020-12-24T17:00:53.716079Z",
     "shell.execute_reply": "2020-12-24T17:00:53.715080Z"
    }
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import tut_utils\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 32\n",
    "PATIENCE = 4  # early stopping\n",
    "NUM_WORKERS = 0 if os.name == 'nt' else 2\n",
    "gpus = 1 if torch.cuda.is_available() else 0\n",
    "\n",
    "results = []\n",
    "for i_iter, test_fold in enumerate(range(K_FOLD)):\n",
    "    logger.info('{i}th iteration of {k}-fold CV'.format(i=i_iter + 1,\n",
    "                                                        k=K_FOLD))\n",
    "    val_fold = (test_fold + 1) % K_FOLD\n",
    "    train_folds = set(range(K_FOLD)) - set([test_fold]) - set([val_fold])\n",
    "    df_train = df_dataset[df_dataset['set'].map(lambda e: e in train_folds)]\n",
    "    df_val = df_dataset[df_dataset['set'] == val_fold]\n",
    "    df_test = df_dataset[df_dataset['set'] == test_fold]\n",
    "    (train_data,\n",
    "     train_labels), (val_data, val_labels), (test_data, test_labels) = [\n",
    "         load_dataset(df, load_img) for df in (df_train, df_val, df_test)\n",
    "     ]\n",
    "    trainloader = DataLoader(AugmentedDataset(train_data, train_labels,\n",
    "                                              random_transform),\n",
    "                             batch_size=BATCH_SIZE,\n",
    "                             shuffle=True,\n",
    "                             num_workers=NUM_WORKERS)\n",
    "    val_loader = DataLoader(AugmentedDataset(val_data, val_labels,\n",
    "                                             base_transform),\n",
    "                            batch_size=BATCH_SIZE,\n",
    "                            shuffle=False,\n",
    "                            num_workers=NUM_WORKERS)\n",
    "\n",
    "    model = LitNet()\n",
    "    early_stop_callback = EarlyStopping(monitor='val_loss',\n",
    "                                        patience=PATIENCE,\n",
    "                                        verbose=False,\n",
    "                                        mode='min')\n",
    "    csv_logger = CSVLogger('train_logs', name='dld')\n",
    "    trainer = pl.Trainer(gpus=gpus,\n",
    "                         max_epochs=EPOCHS,\n",
    "                         logger=csv_logger,\n",
    "                         log_every_n_steps=len(trainloader),\n",
    "                         callbacks=[early_stop_callback])\n",
    "\n",
    "    trainer.fit(model, trainloader, val_loader)\n",
    "    logger.info('Finish training')\n",
    "    df_logs = pd.read_csv(csv_logger.experiment.metrics_file_path)\n",
    "    df_logs = pd.DataFrame(\n",
    "        (df_logs['train_loss'].dropna().reset_index(drop=True),\n",
    "         df_logs['val_loss'].dropna().reset_index(drop=True))).T\n",
    "    df_logs.plot(y=['train_loss', 'val_loss'])\n",
    "    plt.show()\n",
    "    testloader = DataLoader(AugmentedDataset(test_data, test_labels,\n",
    "                                             base_transform),\n",
    "                            batch_size=BATCH_SIZE,\n",
    "                            shuffle=False,\n",
    "                            num_workers=NUM_WORKERS)\n",
    "\n",
    "    df_result = tut_utils.predict_multiclass(model, testloader, df_test.index)\n",
    "    results.append(df_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-24T17:00:53.734060Z",
     "iopub.status.busy": "2020-12-24T17:00:53.720074Z",
     "iopub.status.idle": "2020-12-24T17:00:53.737057Z",
     "shell.execute_reply": "2020-12-24T17:00:53.738056Z"
    }
   },
   "outputs": [],
   "source": [
    "df_result = pd.concat(results, axis=0)\n",
    "df_result = df_dataset.join(df_result)\n",
    "display(df_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 評価\n",
    "### 混同行列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-24T17:00:53.741053Z",
     "iopub.status.busy": "2020-12-24T17:00:53.741053Z",
     "iopub.status.idle": "2020-12-24T17:00:53.748045Z",
     "shell.execute_reply": "2020-12-24T17:00:53.748045Z"
    }
   },
   "outputs": [],
   "source": [
    "df_cm = tut_utils.confusion_matrix(df_result)\n",
    "print('Accuracy = {n} / {d} = {a:.03g}%'.format(n=df_cm.values.trace(),\n",
    "                                                d=df_cm.values.sum(),\n",
    "                                                a=100 * df_cm.values.trace() /\n",
    "                                                df_cm.values.sum()))\n",
    "\n",
    "display(df_cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-24T17:00:53.752041Z",
     "iopub.status.busy": "2020-12-24T17:00:53.752041Z",
     "iopub.status.idle": "2020-12-24T17:00:53.915874Z",
     "shell.execute_reply": "2020-12-24T17:00:53.915874Z"
    }
   },
   "outputs": [],
   "source": [
    "tut_utils.plot_roc_curves(df_result, figsize=(4, 3))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-24T17:00:53.918870Z",
     "iopub.status.busy": "2020-12-24T17:00:53.918870Z",
     "iopub.status.idle": "2020-12-24T17:00:53.929860Z",
     "shell.execute_reply": "2020-12-24T17:00:53.929860Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "report = metrics.classification_report(df_result['class'],\n",
    "                                       df_result['pred_class'],\n",
    "                                       target_names=CLASS_LABELS,\n",
    "                                       output_dict=True)\n",
    "df_report = pd.DataFrame(report)\n",
    "display(df_report.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### クラスごとに間違えている例を表示\n",
    "#### 画像ごとにlossを計算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-24T17:00:53.936852Z",
     "iopub.status.busy": "2020-12-24T17:00:53.936852Z",
     "iopub.status.idle": "2020-12-24T17:00:53.961827Z",
     "shell.execute_reply": "2020-12-24T17:00:53.960827Z"
    }
   },
   "outputs": [],
   "source": [
    "df_result['loss'] = F.cross_entropy(torch.FloatTensor(\n",
    "    df_result['pred_logits']),\n",
    "                                    torch.tensor(df_result['class']),\n",
    "                                    reduction='none').numpy()\n",
    "display(df_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### lossの値が大きい画像を表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-24T17:00:53.966821Z",
     "iopub.status.busy": "2020-12-24T17:00:53.966821Z",
     "iopub.status.idle": "2020-12-24T17:00:54.858908Z",
     "shell.execute_reply": "2020-12-24T17:00:54.858908Z"
    }
   },
   "outputs": [],
   "source": [
    "N_SAMPLES = 2\n",
    "for class_label, group in df_result.groupby('class_label'):\n",
    "    print(class_label)\n",
    "    worst = group.sort_values('loss', ascending=False).head(N_SAMPLES)\n",
    "    worst_data, worst_labels = load_dataset(worst, load_img)\n",
    "    for img, pred_proba in zip(worst_data, worst['pred_proba']):\n",
    "        plt.figure(figsize=(4, 1.5))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(img.squeeze(), cmap='gray')\n",
    "        plt.axis('off')\n",
    "        ax = plt.subplot(1, 2, 2)\n",
    "        pd.DataFrame(pred_proba, index=CLASS_LABELS).plot(ax=ax,\n",
    "                                                          kind='barh',\n",
    "                                                          legend=False)\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "284px",
    "width": "322px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
