{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 肺結節の良悪性判定"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 前準備\n",
    "### 主要パッケージのインポート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データディレクトリの指定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT = pathlib.Path('Data/Images/LIDC_PN')\n",
    "CLASS_LABELS = ('benign', 'malignant')\n",
    "IMAGE_EXT = '.png'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 画像ファイルを基にpd.DataFrameを作成する\n",
    "画像ファイルは`<クラス名>/<ファイル名>`の形式でデータディレクトリ内に用意されている"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset_df(data_root, class_labels, image_ext):\n",
    "    dfs = []\n",
    "    for cls, class_label in enumerate(class_labels):\n",
    "        df = pd.DataFrame(\n",
    "            [(str(p), class_label, cls)\n",
    "             for p in data_root.glob(class_label + '/*' + image_ext)],\n",
    "            columns=['filepath', 'class_label', 'class'])\n",
    "        dfs.append(df)\n",
    "    df_dataset = pd.concat(dfs, ignore_index=True)\n",
    "    return df_dataset\n",
    "\n",
    "\n",
    "df_dataset = create_dataset_df(DATA_ROOT, CLASS_LABELS, IMAGE_EXT)\n",
    "assert set(CLASS_LABELS) == set(df_dataset['class_label'].unique(\n",
    ")), 'Discrepancy between CLASS_LABELS and df_dataset'\n",
    "display(df_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### クラスごとの画像数を確認する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dataset['class_label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 画像を表示してみる\n",
    "各クラスからランダムに選択した画像を表示する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "\n",
    "def show_images_each_class(df, n_rows=2, n_cols=5):\n",
    "    for class_label, group in df.groupby('class_label'):\n",
    "        print(class_label)\n",
    "        for i, row in enumerate(group.sample(n=n_rows * n_cols).itertuples()):\n",
    "            plt.subplot(n_rows, n_cols, i + 1)\n",
    "            image = Image.open(row.filepath)\n",
    "            row.filepath\n",
    "            plt.imshow(image, cmap='gray' if image.mode=='L' else None)\n",
    "            plt.axis('off')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "show_images_each_class(df_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ホールドアウト検証用にデータセットを分割する\n",
    "今回はデータセットの$\\frac{2}{3}$を学習用、$\\frac{1}{3}$を評価用に使用する。\n",
    "分割にはsklearnの[StratifiedKFold](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html)を使う。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "K_FOLD = 3\n",
    "kfold = StratifiedKFold(n_splits=K_FOLD, shuffle=True)\n",
    "train_index, test_index = next(\n",
    "    kfold.split(df_dataset['filepath'], df_dataset['class']))\n",
    "\n",
    "df_train = df_dataset.iloc[train_index]\n",
    "df_test = df_dataset.iloc[test_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データを読み込む\n",
    "読み込んだ画像は０から255の値をとるため読み込んだあとに255で割ることで0から1の値をとるようにする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "IMG_SHAPE = (64, 64, 1)\n",
    "\n",
    "\n",
    "def load_img(filepath):\n",
    "    return np.atleast_3d(\n",
    "        tf.keras.preprocessing.image.load_img(\n",
    "            filepath,\n",
    "            color_mode='grayscale' if IMG_SHAPE[2] == 1 else 'rgb',\n",
    "            target_size=IMG_SHAPE))\n",
    "\n",
    "\n",
    "train_data = np.stack(\n",
    "    [load_img(filepath) for filepath in df_train['filepath']])\n",
    "train_labels = df_train['class']\n",
    "test_data = np.stack([load_img(filepath) for filepath in df_test['filepath']])\n",
    "test_labels = df_test['class']\n",
    "\n",
    "train_data = train_data / 255\n",
    "test_data = test_data / 255\n",
    "\n",
    "print('training data', train_data.shape, train_labels.shape,\n",
    "      train_labels.mean())\n",
    "print('test data', test_data.shape, test_labels.shape, test_labels.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ネットワーク作成\n",
    "今回は画像サイズが小さいためモデルを自作する必要があるが、本来は既存のモデルを流用したほうがよい。\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>注意:</b> BatchNormalizationレイヤーのmomentumのデフォルト値は0.99だが、それだとうまくいかなかったので0.90を指定している。\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(layers.Conv2D(4, 3, activation='relu', input_shape=IMG_SHAPE))\n",
    "model.add(layers.Conv2D(4, 3, activation='relu'))\n",
    "model.add(layers.BatchNormalization(momentum=0.90))\n",
    "model.add(layers.MaxPooling2D(2))\n",
    "model.add(layers.Conv2D(8, 3, activation='relu'))\n",
    "model.add(layers.Conv2D(8, 3, activation='relu'))\n",
    "model.add(layers.BatchNormalization(momentum=0.90))\n",
    "model.add(layers.MaxPooling2D(2))\n",
    "model.add(layers.Conv2D(16, 3, activation='relu'))\n",
    "model.add(layers.Conv2D(16, 3, activation='relu'))\n",
    "model.add(layers.BatchNormalization(momentum=0.90))\n",
    "model.add(layers.MaxPooling2D(2))\n",
    "model.add(layers.Dropout(.25))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(32, activation='relu'))\n",
    "model.add(layers.Dense(1))\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ネットワーク構造の可視化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "from tensorflow.python.keras.utils.vis_utils import plot_model\n",
    "net_arch = tf.keras.utils.model_to_dot(model,\n",
    "                                       show_shapes=True,\n",
    "                                       show_layer_names=False,\n",
    "                                       rankdir='LR',\n",
    "                                       dpi=200).create(prog='dot',\n",
    "                                                       format='png')\n",
    "IPython.display.display_png(IPython.display.Image(net_arch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習\n",
    "### 学習経過をモニタするためのクラスを作成\n",
    "Notebook上で正しく経過を表示するために作成する。チュートリアルではこのクラスを使用するが、通常はverbose=1か2を使えば問題ない。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import tqdm\n",
    "\n",
    "\n",
    "class ProgressBarCallback(tf.keras.callbacks.Callback):\n",
    "    '''Nested progress with single bar\n",
    "  '''\n",
    "    def __init__(self, epochs, n_batches, batch_size, leave=True):\n",
    "        self.n_epochs = epochs\n",
    "        self.n_batches = n_batches\n",
    "        self.batch_size = batch_size\n",
    "        self.bar = tqdm.tqdm(\n",
    "            total=self.n_batches,\n",
    "            leave=leave,\n",
    "            unit='batch',\n",
    "            desc='1/{n_epochs} epoch'.format(n_epochs=self.n_epochs))\n",
    "\n",
    "    def __enter__(self):\n",
    "        return self\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        self.logs = []\n",
    "        self.time_epoch_begin = time.time()\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        self.bar.reset()\n",
    "        summary = pd.DataFrame(self.logs).mean().to_dict()\n",
    "        str_summary = ','.join(\n",
    "            ['{}={:.03g}'.format(k, v) for k, v in summary.items()])\n",
    "        duration = time.time() - self.time_epoch_begin\n",
    "        duration = '{:.02g}m'.format(\n",
    "            duration / 60) if duration >= 100 else '{:.02g}s'.format(duration)\n",
    "        self.bar.set_description(\n",
    "            '{epoch}/{n_epochs} epoch [{duration}/epoch last_epoch=({summary})]'\n",
    "            .format(duration=duration,\n",
    "                    summary=str_summary,\n",
    "                    epoch=epoch + 1,\n",
    "                    n_epochs=self.n_epochs))\n",
    "\n",
    "    def on_batch_begin(self, batch, logs={}):\n",
    "        pass\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.logs.append(logs)\n",
    "        self.bar.update(self.batch_size)\n",
    "        self.bar.set_postfix(logs)\n",
    "\n",
    "    def __exit__(self, exc_type, exc_value, traceback):\n",
    "        self.bar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fit\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>注意:</b> 今回、epoch数は決め打ちしてありますが、本来はvalidationデータを用いて学習を終了させる必要があります。\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8\n",
    "EPOCHS = 16\n",
    "\n",
    "with ProgressBarCallback(EPOCHS,\n",
    "                         len(train_data) // BATCH_SIZE, BATCH_SIZE) as pbar:\n",
    "    result = model.fit(train_data,\n",
    "                       train_labels.values,\n",
    "                       batch_size=BATCH_SIZE,\n",
    "                       epochs=EPOCHS,\n",
    "                       shuffle=True,\n",
    "                       verbose=0,\n",
    "                       callbacks=[pbar])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学習履歴の可視化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(result.history).plot(title='Training history', figsize=(5, 3))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 評価\n",
    "### 混同行列\n",
    "学習できているかを確認するため、まずは学習データでの評価を行う。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "def evaluate(model, data, labels):\n",
    "    predictions = tf.nn.sigmoid(model.predict(data)).numpy().squeeze()\n",
    "    y_pred = predictions > .5\n",
    "    df_result = pd.DataFrame({\n",
    "        'truth': labels,\n",
    "        'pred_proba': predictions,\n",
    "        'pred_class': y_pred\n",
    "    })\n",
    "    cm = metrics.confusion_matrix(df_result['truth'], df_result['pred_class'])\n",
    "    df_cm = pd.DataFrame(cm, index=CLASS_LABELS, columns=CLASS_LABELS)\n",
    "    df_cm.index.name, df_cm.columns.name = 'Truth', 'Prediction'\n",
    "    display(df_cm)\n",
    "    print('Accuracy = {n} / {d} = {a:.03g}%'.format(n=cm.trace(),\n",
    "                                                    d=cm.sum(),\n",
    "                                                    a=100 * cm.trace() /\n",
    "                                                    cm.sum()))\n",
    "    return df_result\n",
    "\n",
    "\n",
    "train_result = evaluate(model, train_data, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "評価データでの評価を行う。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result = evaluate(model, test_data, test_labels.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROCカーブ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = metrics.roc_curve(test_result['truth'],\n",
    "                                         test_result['pred_proba'])\n",
    "auc = metrics.auc(fpr, tpr)\n",
    "plt.figure(figsize=(3, 3))\n",
    "plt.plot(fpr, tpr, label='AUC = {auc:.03g}'.format(auc=auc))\n",
    "plt.plot((0, 1), (0, 1), zorder=0, color='black', alpha=.1,\n",
    "         linestyle='-')  # diagonal line\n",
    "plt.xlabel('1 - Specificity')\n",
    "plt.ylabel('Sensitivity')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
